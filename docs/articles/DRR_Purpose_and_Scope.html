<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>DRR Purpose and Scope • QCkit</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="DRR Purpose and Scope">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">QCkit</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/DRR_Purpose_and_Scope.html">DRR Purpose and Scope</a></li>
    <li><a class="dropdown-item" href="../articles/Starting-a-DRR.html">Starting a DRR</a></li>
    <li><a class="dropdown-item" href="../articles/Using-the-DRR-Template.html">Using the DRR Template</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/nationalparkservice/QCkit/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>DRR Purpose and Scope</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/nationalparkservice/QCkit/blob/HEAD/vignettes/articles/DRR_Purpose_and_Scope.Rmd" class="external-link"><code>vignettes/articles/DRR_Purpose_and_Scope.Rmd</code></a></small>
      <div class="d-none name"><code>DRR_Purpose_and_Scope.Rmd</code></div>
    </div>

    
    
<!-- badges: start -->
<p><a href="https://www.tidyverse.org/lifecycle/#experimental" class="external-link"><img src="https://img.shields.io/badge/lifecycle-experimental-orange.svg" alt="Lifecycle: experimental"></a></p>
<!-- badges: end -->
<div class="section level2">
<h2 id="background">Background<a class="anchor" aria-label="anchor" href="#background"></a>
</h2>
<p>The Data Release Report (DRR) is aimed at fulfilling requirements and
expectations of Open Science at the National Park Service. This
includes:</p>
<ul>
<li><p>Broad adoption of open-data and open-by-default
practices.</p></li>
<li><p>A move in the scientific disciplines toward considering and
publishing data sets as independently-citable scientific works.</p></li>
<li><p>Routine assignment of digital object identifiers (DOIs) to
datasets to facilitate location, reuse, and citation of specific
data</p></li>
<li><p>Increased transparency and reproducibility in the processing and
analysis of data.</p></li>
<li><p>Establishment of peer reviewed “data journals” dedicated to
publishing data sets and associated documentation designed to facilitate
their reuse.</p></li>
<li><p>Expectation that science-based decisions are based on
peer-reviewed, reproducible, and open science by default.</p></li>
</ul>
<p>Data Release Reports are designed to parallel external peer-reviewed
scientific journals dedicated to facilitate reuse of reproducible
scientific data, in recognition that the primary reason IMD data are
collected is to support science-based decisions.</p>
<p>Note that publication in a Data Release Report Series (not mandated)
is distinct from requirements to document data collection, processing,
and quality evaluation (mandated; see below). The establishment of a
Data Release Report Series is intended to facilitate and encourage this
type of reporting in a standard format, and in a manner commensurate
with current scientific norms.</p>
</div>
<div class="section level2">
<h2 id="definitions">Definitions<a class="anchor" aria-label="anchor" href="#definitions"></a>
</h2>
<p><strong>Reproducibility.</strong> The degree to which scientific
information, modeling, and methods of analysis could be evaluated by an
independent third party to arrive at the same, or substantially similar,
conclusion as the original study or information, and that the scientific
assessment can be repeated to obtain similar results (Plesser 2017). A
study is reproducible if you can take the original data and the computer
code used to analyze the data and reproduce all of the numerical
findings from the study. This may initially sound like a trivial task
but experience has shown that it’s not always easy to achieve this
seemingly minimal standard (ASA 2017, Plesser 2017).</p>
<p><strong>Transparency.</strong> Full disclosure of the methods used to
obtain, process, analyze, and review scientific data and other
information products, the availability of the data that went into and
came out of the analysis, and the computer code used to conduct the
analysis. Documenting this information is crucial to ensure
reproducibility and requires, at minimum, the sharing of analytical data
sets, relevant metadata, analytical code, and related software.</p>
<p><strong>Fitness for Use.</strong> The utility of scientific
information (in this case a dataset) for its intended users and its
intended purposes. Agencies must review and communicate the fitness of a
dataset for its intended purpose, and should provide the public
sufficient documentation about each dataset to allow data users to
determine the fitness of the data for the purpose for which third
parties may consider using it.</p>
<p><strong>Decisions.</strong> The type of decisions that must be based
on publicly-available, reproducible, and peer-reviewed science has not
been defined. At a minimum it includes any influential decisions, but it
may also include any decisions subject to public review and comment.</p>
<p><strong>Descriptive Reporting.</strong> The policies listed above are
consistent in the requirement to provide documentation that describes
the methods used to collect, process, and evaluate science products,
including data. Note that this is distinct from (and in practice may
significantly differ from) prescriptive documents such as protocols,
procedures, and study plans. Descriptive reporting should cite or
describe relevant science planning documents, methods used, deviations,
and mitigations. In total, descriptive reporting provides a clear “line
of sight” on precisely how data were collected, processed, and
evaluated. Although deviations may warrant revisions to prescriptive
documents, changes in prescriptive documents after the fact do not meet
reproducibility and transparency requirements.</p>
</div>
<div class="section level2">
<h2 id="policy-requirements">Policy Requirements<a class="anchor" aria-label="anchor" href="#policy-requirements"></a>
</h2>
<div class="section level3">
<h3 id="nps-requirements">NPS Requirements<a class="anchor" aria-label="anchor" href="#nps-requirements"></a>
</h3>
<p><a href="https://www.nps.gov/subjects/policy/upload/DO_11B_10-16-2002.pdf" class="external-link"><strong>DO11B-a</strong></a><strong>,
<a href="https://irma.nps.gov/DataStore/DownloadFile/474958" class="external-link">DO
11B-b</a>, <a href="https://www.whitehouse.gov/wp-content/uploads/2017/11/2005-M-05-03-Issuance-of-OMBs-Final-Information-Quality-Bulletin-for-Peer-Review-December-16-2004.pdf" class="external-link">OMB
M-05-03</a> (Peer review and information quality):</strong></p>
<ul>
<li><p>Scientific information must be appropriately reviewed prior to
use in decision-making, regulatory processes, or dissemination to the
public, regardless of media.</p></li>
<li><p>As per OMB M-05-03 “scientific information” includes factual
inputs, data, models, analyses, technical information, or scientific
assessments related to such disciplines as the behavioral and social
sciences, public health and medical sciences, life and earth sciences,
engineering, or physical sciences.</p></li>
<li><p>Methods for producing information will be made transparent, to
the maximum extent practicable, through accurate documentation, use of
appropriate review, and verification of information quality.</p></li>
</ul>
<p><a href="https://www.whitehouse.gov/wp-content/uploads/2019/04/M-19-15.pdf" class="external-link"><strong>OMB
M-19-15</strong></a> <strong>(Updates to Implementing the Information
Quality Act):</strong></p>
<ul>
<li><p>Federal agencies must collect, use, and disseminate information
that is fit for its intended purpose.</p></li>
<li><p>Agencies must conduct pre-dissemination review of quality [of
scientific information] based on the likely use of that information.
Quality encompasses utility, integrity, and objectivity, defined as
follows: a) Utility – utility for its intended users and its intended
purposes, b) Integrity – refers to security, and c) Objectivity –
accurate, reliable, and unbiased as a matter of presentation and
substance.</p></li>
<li><p>Agencies should provide the public with sufficient documentation
about each dataset released to allow data users to determine the fitness
of the data for the purpose for which third parties may consider using
it. Potential users <em>must</em> be provided with sufficient
information to understand… the data’s strengths, weaknesses, analytical
limitations, security requirements, and processing options.</p></li>
<li>
<p>Reproducibility requirements for Influential Information. Note
that because this may not be determined at the time of collection,
processing, or dissemination this should be the default for NPS
scientific activities:</p>
<ul>
<li><p>Analyses must be disseminated with sufficient descriptions of
data and methods to allow them to be reproduced by qualified third
parties who may want to test the sensitivity of agency analyses. This is
a higher standard than simply documenting the characteristics of the
underlying data, which is required for all information.</p></li>
<li><p>Computer code used to process data should be made available to
the public for further analysis. In the context of results generated,
for example, a statistical model or machine augmented learning and
decision support, reproducibility requires, at a minimum transparency
about the specific methods, design parameters, equations or algorithms,
parameters, and assumptions used.</p></li>
</ul>
</li>
<li><p>Reports, data, and computer code used, developed, or cited in the
analysis and reporting of findings must be made publicly available
except where prohibited by law.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="national-park-service-guidelines">National Park Service Guidelines<a class="anchor" aria-label="anchor" href="#national-park-service-guidelines"></a>
</h3>
<p>Multiple policy and guidance documents require the use of best
available science in decision-making at the Natonal Park Service (NPS).
Additional requirements include:</p>
<p><a href="https://www.doi.gov/sites/doi.gov/files/elips/documents/so_3369_promoting_open_science.pdf" class="external-link"><strong>SO
3369</strong></a> <strong>(Promoting Open Science):</strong></p>
<ul>
<li>Defines “best available science” as publicly-available,
reproducible, and peer reviewed. Requires that any decisions or
scientific conclusions must prioritize the use of publicly-available,
reproducible, and peer-reviewed science. Decisions or conclusions not
based on such must include an explanation of why the alternative is the
best available information. Effective as of 28 September 2018 with no
transition period.</li>
</ul>
<p><a href="https://www.nps.gov/subjects/policy/upload/DO_11B_10-16-2002.pdf" class="external-link"><strong>DO
11B</strong></a> <strong>(Ensuring Objectivity, Utility, and Integrity
of Information Used and Disseminated by the National Park
Service):</strong></p>
<ul>
<li>The NPS will ensure that information it releases to the public or
utilizes in management decisions will be developed from reliable data
sources that provide the highest quality of information at each stage of
information development.</li>
</ul>
</div>
<div class="section level3">
<h3 id="nps-inventory-and-monitoring-requirements">NPS Inventory and Monitoring Requirements<a class="anchor" aria-label="anchor" href="#nps-inventory-and-monitoring-requirements"></a>
</h3>
<p><a href="https://irma.nps.gov/DataStore/Reference/Profile/622933" class="external-link"><strong>NPS-75</strong></a>
<strong>(Inventory and Monitoring Guidelines):</strong></p>
<ul>
<li><p>An annual summary report documenting the condition of park
resources should be developed as part of the annual revision of the
parks Resource Management Plan.</p></li>
<li><p>An annual report provides a mechanism for reviewing and making
recommendations for revisions in the [Protocol/SOPs].</p></li>
<li><p>[Inventory] data obtained should be archived in park records and,
when appropriate, a report should be written summarizing
findings.</p></li>
<li><p>Reporting requirements as per IMD directive</p></li>
</ul>
<p><a href="https://irma.nps.gov/DataStore/Reference/Profile/2252934" class="external-link"><strong>IMD
Reporting and Analysis Guidance</strong></a></p>
<ul>
<li>Annual Analyses Required of all Monitoring Protocols: Conduct an
annual data review to address whether there is any unexpected
variability or outliers, and whether any protocol changes or additional
studies may be needed. Part of the review must assess the data against
standards defined in the protocol narrative, data quality standards
document or quality assurance plan, and document whether those standards
were met. When data are not available for review during the year they
are collected (for example, when data have been submitted to a lab for
analysis), review must be conducted the year the data are available. For
example, if water quality and quantity data are typically reviewed in
October and lab results for water quality are not available until the
following March, these data must be reviewed during the following
October review period, if not before.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="implications">Implications<a class="anchor" aria-label="anchor" href="#implications"></a>
</h2>
<p>Because all of the data the NPS IMD collects is intended for use in
supporting science-based decisions as per our program’s five goals, and
is intended for use in planning (the decisions of which are subject to
public comment as per NEPA requirements), this means that by
default:</p>
<ul>
<li><p>All analytical work we do should be reproducible to the extent
possible. Analytical work includes both statistical analysis and
reporting of data as well as quality control procedures where data are
tested against quality standards and qualified or corrected as
appropriate.</p></li>
<li><p>Full reproducibility may not be possible in all cases,
particularly where analytical methods involve subject matter expertise
to make informed judgments on how to proceed with analyses. In such
cases, decisions should be documented to ensure transparency.</p></li>
<li><p>All IMD data should be published with supporting documentation to
allow for reproduction of results.</p></li>
<li><p>All IMD data should be evaluated to determine whether they are
suitable for their intended use.</p></li>
<li><p>All IMD data should be published with information fully
describing how data were collected, processed, and evaluated.</p></li>
<li><p>All data should be published in open formats that support the <a href="https://www.go-fair.org/fair-principles/" class="external-link">FAIR principles</a>
(Findable, Accessible, Interoperable, and Resuable).</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="scope">Scope<a class="anchor" aria-label="anchor" href="#scope"></a>
</h2>
<p>(for the NPS Inventory &amp; Monitoring Program)</p>
<div class="section level3">
<h3 id="general-studies">General Studies<a class="anchor" aria-label="anchor" href="#general-studies"></a>
</h3>
<p>Any project that involves the collection of scientific data for use
in supporting decisions to be made by NPS personnel. General study data
may or may not be collected based on documented or peer-reviewed study
plans or defined quality standards, but are in most cases purpose-driven
and resultant information should be evaluated for the suitability
for—and prior to—their use in decision support. These data may be reused
for secondary purposes including similar decisions at other locations or
times and/or portions of general study data may be reused or contribute
to other scientific work (observations from a deer browsing study may be
contribute to an inventory or may be used as ancillary data to explain
monitoring observations).</p>
<div class="figure" style="text-align: center">
<img src="WorkflowModelGeneral.png" alt="Workflow for data collection, processing, dissemination, and use for general studies. Teal-colored boxes are subject to reproducibility requirements." width="70%"><p class="caption">
Workflow for data collection, processing, dissemination, and use for
general studies. Teal-colored boxes are subject to reproducibility
requirements.
</p>
</div>
</div>
<div class="section level3">
<h3 id="vital-signs-monitoring">Vital Signs Monitoring<a class="anchor" aria-label="anchor" href="#vital-signs-monitoring"></a>
</h3>
<p>Vital signs monitoring data are collected by IMD and park staff to
address specific monitoring objectives following methods designed to
ensure long-term comparability of data. Procedures are established to
ensure that data quality standards are maintained in perpetuity.
However, because monitoring data are collected over long periods of time
in dynamic systems, the methods employed may differ from those
prescribed in monitoring protocols, procedures, or sampling plans, and
any deviations (and resultant mitigations to the data) must be
documented. Data should be evaluated to ensure that they meet prescribed
standards and are suitable for analyses designed to test whether
monitoring objectives have been met. Monitoring data may be reused for
secondary purposes including synthesis reports and condition
assessments, and portions of monitoring data may contribute to
inventories.</p>
<div class="figure" style="text-align: center">
<img src="WorkflowModelMonitoring.png" alt="Workflow for data collection, processing, dissemination, and use for vital sign monitoring efforts. Teal-colored boxes are subject to reproducibility requirements." width="70%"><p class="caption">
Workflow for data collection, processing, dissemination, and use for
vital sign monitoring efforts. Teal-colored boxes are subject to
reproducibility requirements.
</p>
</div>
</div>
<div class="section level3">
<h3 id="inventory-studies">Inventory Studies<a class="anchor" aria-label="anchor" href="#inventory-studies"></a>
</h3>
<p>Inventory study data are similar to general study data in that they
are time- and area-specific efforts designed to answer specific
management needs as well as broader inventory objectives outlined in
project-specific study plans and inventory science plans. Inventory
studies typically follow well-documented data collection methods or
procedures, and resultant data should be evaluated for whether they are
suitable for use in supporting study-specific and broader
inventory-level objectives. Inventory study data are expected to be
reused to meet broader inventory level goals, but may also support other
scientific work and decision support.</p>
<div class="figure" style="text-align: center">
<img src="WorkflowModelInventories.png" alt="Workflow for data collection, processing, dissemination, and use for inventory studies. Teal-colored boxes are subject to reproducibility requirements." width="70%"><p class="caption">
Workflow for data collection, processing, dissemination, and use for
inventory studies. Teal-colored boxes are subject to reproducibility
requirements.
</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>American Statistical Association (ASA). 2017. Recommendations to
funding agencies for supporting reproducible research. <a href="https://www.amstat.org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf" class="external-link uri">https://www.amstat.org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf</a>.</p>
<p>Plesser, H. E. 2017. Reproducibility vs. Replicability: A brief
history of a confused terminology. Front. Neuroinform. 11:76. <a href="https://doi.org/10.3389/fninf.2017.00076" class="external-link uri">https://doi.org/10.3389/fninf.2017.00076</a>.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Robert Baker, Judd Patterson, Joe DeVivo, Issac Quevedo, Sarah Wright.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
